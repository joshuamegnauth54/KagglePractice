---
title: "Kaggle Admissions2 Data Test"
author: "Joshua Megnauth"
output:
  html_document:
    theme: darkly
    highlight: zenburn
    df_print: paged
---

## Introduction
```{r load_libraries_data, warning=FALSE, message=FALSE}
library(kableExtra)
library(parallel)
library(doParallel)
library(ranger)
library(caret)
library(tidyverse)
library(ggplot2)
library(broom)

# Dracula colors
# https://draculatheme.com/
# Obviously not following the theme yet.
PINK <- "#ff79c6"
CYAN <- "#8be9fd"
GREEN <- "#50fa7b"
PURPLE <- "#bd93f9"
RED <- "#ff5555"
BACKGROUND <- "#282a36"

# Threshold for the outcomes (i.e. above THRESHOLD = a success)
THRESHOLD <- 0.65

admissions_name <- "Admission_Predict_Ver1.1.csv"
admissions_cols <- c("id", "gre", "toefl", "uni_ratings", "statement",
                     "letter", "cgpa", "research", "prob_admit")
admissions_types <- "iiifdddld"

admissions <- 
  read_csv(admissions_name, col_names = admissions_cols,
           col_types = admissions_types, skip = 1) %>%
  select(-id)

cluster_mp <- makeCluster(detectCores() - 1)
registerDoParallel(cluster_mp)

options(scipen = 16)
set.seed(314)
rm(admissions_name, admissions_cols, admissions_types)
```

The [graduate admissions data](https://www.kaggle.com/mohansacharya/graduate-admissions) data set is not immediately attractive at first glance. The data set is sparse, utilitarian, and tractable. Utility is a function of the data's admittedly humdrum goal of predicting graduate school admissions from dreary tests. However, the data set is also yielding due to the directness of the variables and the lack of cleaning required.

For those reasons I had been drawn to this data as a first, small project to play around with in R and Python.

Let's quickly go over the code above. **THRESHOLD** is a constant to separate the positive predictions from the negative---that is, admissions from non-admissions. I selected 0.65 as a probability after exploring the data below which means that the value is a judgement call rather than a science. As a parameter, a statistician or data scientist may tune the threshold for separating the classes. Finally, .65 is hardly perfect as we'll see below.

Anyway, the code to load the data isn't all that interesting. I manually skipped the header row, renamed the columns, and set the types.

As of 8/2/2020 I have set a seed of **314.** My random seed will always be 324, 314, 428, or 42. My code is open source with each commit available on GitHub to confirm I am not tampering with random seeds.

## Exploring the data
```{r tests_plots}
standardize <- function(variable) {
  (variable - mean(variable))/sd(variable)
}

thresh_to_fac <- function(data, threshold = THRESHOLD) {
  as.factor(data > threshold)
}

train_test_split <- function(X, y, test_size = .2) {
  len <- length(X)
  rows <- sample(len)
  split <- test_size * len
  
  X_test <- X[1:split]
  y_test <- y[1:split]

  X_train <- X[split + 1:len]
  y_train <- y[split + 1:len]
  
  list(X_train, y_train, X_test, y_test)
}

admissions %>%
  mutate(gre_stdize = standardize(gre),
         toefl_stdize = standardize(toefl)) %>%
  ggplot() +
  geom_histogram(aes(gre_stdize), binwidth = .25,
                 color = BACKGROUND, fill = GREEN) +
  geom_histogram(aes(toefl_stdize), binwidth = .25,
                 color = BACKGROUND, fill = PINK, alpha = 0.5) +
  xlab("GRE and TOEFL (standardized)") +
  ylab("Frequency") +
  ggtitle("Standardized GRE and TOEFL scores")
```

$$z = \frac{x - \mu}{\sigma}$$

[to do]
Explain the standardization formula.

```{r probability_admission}
admissions %>%
  ggplot() +
  geom_density(aes(prob_admit, fill = research), alpha = 0.5) +
  scale_fill_manual(values = c(PINK, CYAN)) +
  labs(fill = "Research") +
  xlab("Admission") +
  ylab("Probability") +
  ggtitle("Probability of admission by research experience")
```

The probability of admission seems normally distributed for students without research experience while positively skewed for students with projects. Both modes peak past the midway point of 0.5. My initial guess is that setting a threshold of 0.65 would catch more _true positives_ than using 0.5. I wouldn't happily cartwheel around my room at 50-50 odds for school admissions. Likewise, a strict separation at 50% could risk incorrect predictions especially when reasonable students wouldn't bet on such low odds.

A higher threshold logically risks more false negatives---cases where students would be admitted but the model predicted rejection. However, a greater false negatives rate is less risky than false positives here in my opinion. Applying to schools only to be declined seems more individually taxing than false negatives unless the rate for the latter is unreasonbly high.

```{r}
admissions %>%
  select(gre, toefl, statement, letter) %>%
  rename_all(~list(gre = "GRE", toefl = "TOEFL", statement = "SoP",
                   letter = "LoR")) %>%
  cor() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## A basic model

```{r naive_glm}
admissions <- admissions %>%
  mutate(y_admit = thresh_to_fac(prob_admit))

basic_form <- y_admit ~ gre + uni_ratings + cgpa + statement + letter + research
basic_mod <- glm(basic_form, "binomial", admissions)

tidy(basic_mod, conf.int = TRUE, exponentiate = TRUE)
```

My first model is conservative and straightforward. I included only one of the tests as the two test scores are correlated. Of course, a person/observation that has a certain score on one test is likely to have a similar grade on the other test. Likewise, G.P.A. shouldn't deviate too wildly from TOEFL or GRE scores, but I included the variable in this preliminary model to be safe.

The coefficients for the odds fall into the confidence intervals. The confidence intervals are a bit wonky. For example, the confidence intervals for _university ratings_ are rather wide. P-values measure the probability of results at least as extreme as what was observed given that the null hypothesis is true. Only GRE scores, cumulative G.P.A., and letters of recommendation scores may not be due to random chance given $\alpha = .05$.

### Diagnostics

Let's test predictive ability. I fit my basic model on all of the data which in turn will be used for prediction. Of course, I realize that precluding a hold out set as well as using training data for prediction are anti-patterns, but the model above is basic and experimentation is fun!

```{r basic_model_test}
confusion_wrapper <- function(y_true, y_pred) {
  confusion <- confusionMatrix(y_pred, reference = y_true, positive = "TRUE")
  
  confusion$table %>%
    kable(caption = "Confusion matrix: columns = true") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                  full_width = FALSE)
  
  tidy(confusion) %>%
    select(term, estimate) %>%
    kable(col.names = c("Classification Function", "Value")) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
}

basic_preds <- thresh_to_fac(predict(basic_mod, type = "response"))
confusion_wrapper(admissions$y_admit, basic_preds)
```

My basic model performs _okay_ considering I conservatively hand picked variables. With that said, I'll repeat that I _tested_ on my _training_ data so the model may be overfit with only decent performance. Total accuracy is the ratio of correctly separated true positive and true negatives over all of the classes total. Accuracy's major problem is that the two classes may be unbalanced. The canonical example considers the test for an affliction where most test results are negative. Let's say that 99% of the cases in a data set for some condition are negative. A classifier may achieve an accuracy score of 99% while predicting **literally every positive case incorrectly.**

The logistic model above achieves an accuracy of 86% with a sensitivity (true positive rate) of 93.6%. Specificied, or correctly sorting false cases, is only 73.4%. The confusion matrix above shows 20 false negatives and 50 false positives. I set a threshold of 0.65 as discussed earlier. However, that high threshold probably borked the sorting ability of the model considering that the data is skewed towards positive cases now.

```{r cases_thresh_sixtyfive}
table(admissions$y_admit) %>%
  kable(col.names = c("Class", "Frequency")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

How about we check model performance with the threshold set at 0.5? For fun (and profit)!

```{r log_thresh_fifty}
# This code is dreadful. Sorry.
fifty_pred <- admissions %>%
  mutate(y_true = thresh_to_fac(prob_admit, threshold = .5)) %>%
  glm(basic_form, "binomial", data = .) %>%
  predict(type = "response") %>%
  thresh_to_fac(threshold = .5)

fifty_true <- thresh_to_fac(admissions$prob_admit, .5)
confusion_wrapper(fifty_true, fifty_pred)

rm(fifty_pred, fifty_true)
```

Using my handy dandy ~~notebook~~ _thresh_to_fac()_ function, I generated a set of predictions as factors where the positive class is defined as 0.51 and up.

My initial intuition from data exploration has been (somewhat) vindicated! I did not expect the model to perform so poorly with that change. Specificity is extremely low which is rather sensible considering the skewness of the classes. I'll keep 0.65 as a threshold.

```{r drop_prob_admit}
admissions <-
  select(-prob_admit)
```

## Bootstrap aggregated random forests [Baggin' CARTs]

```{r bagging_rf}
rf_traincontrol <- trainControl(method = "LOOCV",
                                search = "random",
                                classProbs = TRUE,
                                summaryFunction = twoClassSummary,
                                allowParallel = TRUE)

mtry_default <- floor(sqrt(ncol(admissions) - 1))
rf_grid <- expand.grid(mtry = c(1, mtry_default, 3, 6),
                       min.node.size = c(1, 3, 5, 10))

rf_model_nopreproc <- train(y_admit ~ .,
                            method = "ranger",
                            trControl = rf_traincontrol,
                            tuneGrid = rf_grid,
                            metric = "ROC",
                            data = admissions)
```

The two highest rated answers on a question about [preprocessing for random forests] on Stack Overflow conflict. Wonderful! Random forests should theoretically be indifferent toward outliers or unimportant variables. One answer describes the algorithm as fairly "off the shelf," meaning that one may deploy a CART without much wrangling. Another answer explains that the theoretical prowess is sometimes wonky; all data should be seasoned a bit before cooking. I'll try the model both ways. Preprocessing shouldn't hurt anything, but less complex models or modeling processes should always be preferred of course.

I'm using _leave one out cross validation_ as the data are rather limited at `r nrow(admissions)` rows. I'm avoiding splitting the data into training and test sets for that reason as well.

The hyperparameter _mtry_ refers to the number of variables that may be possibly split at each intersection.

```{r bagging_preproc_rf}
rf_model_preproc <- train(y_admit ~ .,
                          method = "ranger",
                          preProcess = c("scale", "pca"),
                          trControl = rf_traincontrol,
                          tuneGrid = rf_grid,
                          metric = "ROC",
                          data = admissions)
```

```{r stop_cluster}
stopCluster(cluster_mp)
```
