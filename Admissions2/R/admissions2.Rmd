---
title: "Kaggle Admissions2 Data Test"
author: "Joshua Megnauth"
output:
  html_document:
    theme: darkly
    highlight: zenburn
    df_print: paged
---

## Introduction
```{r load_libraries_data, warning=FALSE, message=FALSE}
library(kableExtra)
library(parallel)
library(doParallel)
library(ranger)
library(caret)
library(tidyverse)
library(ggplot2)
library(broom)

# Dracula colors
# https://draculatheme.com/
# Obviously not following the theme yet.
PINK <- "#ff79c6"
CYAN <- "#8be9fd"
GREEN <- "#50fa7b"
PURPLE <- "#bd93f9"
RED <- "#ff5555"
BACKGROUND <- "#282a36"

# Threshold for the outcomes (i.e. above THRESHOLD = a success)
THRESHOLD <- 0.65

admissions_name <- "Admission_Predict_Ver1.1.csv"
admissions_cols <- c("id", "gre", "toefl", "uni_ratings", "statement",
                     "letter", "cgpa", "research", "prob_admit")
admissions_types <- "iiifdddld"

admissions <- 
  read_csv(admissions_name, col_names = admissions_cols,
           col_types = admissions_types, skip = 1) %>%
  select(-id) %>%
  mutate(research = as_factor(if_else(research, "Yes", "No")))

cluster_mp <- makeCluster(detectCores() - 1)
registerDoParallel(cluster_mp)

options(scipen = 16)
set.seed(314)
rm(admissions_name, admissions_cols, admissions_types)
```

The [graduate admissions data](https://www.kaggle.com/mohansacharya/graduate-admissions) data set is not immediately attractive at first glance. The data set is sparse, utilitarian, and tractable. Utility is a function of the data's admittedly humdrum goal of predicting graduate school admissions from dreary tests. However, the data set is also yielding due to the directness of the variables and the lack of cleaning required.

For those reasons I had been drawn to this data as a first, small project to play around with in R and Python.

Let's quickly go over the code above. **THRESHOLD** is a constant to separate the positive predictions from the negative---that is, admissions from non-admissions. I selected 0.65 as a probability after exploring the data below which means that the value is a judgement call rather than a science. As a parameter, a statistician or data scientist may tune the threshold for separating the classes depending on the teleology of the problem. Finally, 0.65 is hardly perfect, but I found the value to perform well.

Anyway, the code to load the data isn't all that interesting. I manually skipped the header row, renamed the columns, and set the types. I converted _research_ to a factor with the levels renamed as TRUE and FALSE are invalid names in R. [Caret](https://topepo.github.io/caret/) throws an exception when invalid names are used. I'm also putting my [i7](https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors) to work with multiprocessing.

As of 8/2/2020 I have set a seed of **314.** My random seed will always be 324, 314, 428, or 42. My code is open source with each commit available on GitHub to confirm I am not tampering with random seeds.

## Exploring the data
```{r tests_plots}
standardize <- function(variable) {
  (variable - mean(variable))/sd(variable)
}

thresh_to_fac <- function(data, threshold = THRESHOLD) {
  # Factor levels are created by order, so I need to ensure the order for
  # the negative class is explicit.
  temp <- as_factor(if_else(data > threshold, "Yes", "No"))
  relevel(temp, "No")
}

train_test_split <- function(X, y, test_size = .2) {
  len <- length(X)
  rows <- sample(len)
  split <- test_size * len
  
  X_test <- X[1:split]
  y_test <- y[1:split]

  X_train <- X[split + 1:len]
  y_train <- y[split + 1:len]
  
  list(X_train, y_train, X_test, y_test)
}

admissions %>%
  mutate(gre_stdize = standardize(gre),
         toefl_stdize = standardize(toefl)) %>%
  ggplot() +
  geom_histogram(aes(gre_stdize), binwidth = .25,
                 color = BACKGROUND, fill = GREEN) +
  geom_histogram(aes(toefl_stdize), binwidth = .25,
                 color = BACKGROUND, fill = PINK, alpha = 0.5) +
  xlab("GRE [green] and TOEFL [pink]") +
  ylab("Frequency") +
  ggtitle("Standardized GRE and TOEFL scores")
```

I standardized the variables for GRE and TOEFL scores due to the different scales. Working with standardized or normalized variables is often just plain easier. The process of standardizing a variable is simple. We'd like a standard deviation of 1 and a mean of zero, so we can simple substract the mean of the variable from each score and divide by the standard deviation. In other words, we're taking the z score of each value.

$$z = \frac{x - \mu}{\sigma}$$

The distributions for GRE and TOEFL scores seem fairly similar as we can see from the histograms. In some cases, such as the range from a standard deviation of **0 to 1**, the distributions are near identical.

```{r probability_admission}
admissions %>%
  ggplot() +
  geom_density(aes(prob_admit, fill = research), alpha = 0.5) +
  scale_fill_manual(values = c(PINK, CYAN)) +
  labs(fill = "Research") +
  xlab("Admission") +
  ylab("Probability") +
  ggtitle("Probability of admission by research experience")
```

The probability of admission seems normally distributed for students without research experience while positively skewed for students with projects. Both modes peak past the midway point of 0.5. My initial guess is that setting a threshold of 0.65 would catch more _true positives_ than using 0.5. I wouldn't happily cartwheel around my room at 50-50 odds for school admissions. Likewise, a strict separation at 50% could risk incorrect predictions especially when reasonable students wouldn't bet on such low odds.

A higher threshold logically risks more false negatives---cases where students would be admitted but the model predicted rejection. However, a greater false negatives rate is less risky than false positives here in my opinion. Applying to schools only to be declined seems more individually taxing than false negatives unless the rate for the latter is unreasonbly high.

```{r}
admissions %>%
  select(gre, toefl, statement, letter) %>%
  rename(GRE = gre, TOEFL = toefl, SoP = statement,
         LoR = letter) %>%
  cor() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## A basic model

```{r naive_glm}
admissions <- admissions %>%
  mutate(y_admit = thresh_to_fac(prob_admit))

basic_form <- y_admit ~ gre + uni_ratings + cgpa + statement + letter + research
basic_mod <- glm(basic_form, "binomial", admissions)

tidy(basic_mod, conf.int = TRUE, exponentiate = TRUE)
```

My first model is conservative and straightforward. I included only one of the tests as the two test scores are correlated. Of course, a person/observation that has a certain score on one test is likely to have a similar grade on the other test. Likewise, G.P.A. shouldn't deviate too wildly from TOEFL or GRE scores, but I included the variable in this preliminary model to be safe.

The coefficients for the odds fall into the confidence intervals. The confidence intervals are a bit wonky. For example, the confidence intervals for _university ratings_ are rather wide. P-values measure the probability of results at least as extreme as what was observed given that the null hypothesis is true. In other words, the p-value is concerned with only the probability of extreme events in terms of the _data_ along with assuming the null hypothesis of no effect; we're not considering external information, and the model says nothing about truthiness. Only the odds coefficient of GRE scores, cumulative G.P.A., and letters of recommendation scores may not be due to random chance given $\alpha = .05$.

### Diagnostics

Let's test predictive ability. I fit my basic model on all of the rows of data which in turn will be reused for prediction. Of course, I realize that precluding a hold out set as well as using training data for prediction are anti-patterns, but the model above is basic and experimentation is fun!

```{r basic_model_test, results="asis"}
confusion_wrapper <- function(y_true, y_pred) {
  confusion <- confusionMatrix(y_pred, reference = y_true, positive = "Yes")
  
  cmatrix <-
    confusion$table %>%
    kable(caption = "Confusion matrix: columns = true") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                  full_width = FALSE)
  
  metrics <-
    tidy(confusion) %>%
    select(term, estimate) %>%
    kable(col.names = c("Classification Function", "Value")) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
  
  list(cmatrix, metrics)
}

basic_preds <- thresh_to_fac(predict(basic_mod, type = "response"))
confusion_wrapper(admissions$y_admit, basic_preds)
```

My basic model performs _pretty good_ considering I conservatively hand picked variables. With that said, I'll repeat that I _tested_ on my _training_ data so the model may be overfit with only decent performance. Total accuracy is the ratio of correctly separated true positive and true negatives over all of the classes total. Accuracy's major problem is that the two classes may be unbalanced. The canonical example considers the test for an affliction where most test results are negative. Let's say that 99% of the cases in a data set for some condition are negative. A classifier may achieve an accuracy score of 99% while predicting **literally every positive case incorrectly.**

The logistic model above achieves an accuracy of 86% with a sensitivity (true positive rate) of 85.4%. Specificity, or correctly sorting false cases, is similar at 87.4%. The confusion matrix above shows 20 false negatives and 50 false positives. I set a threshold of 0.65 as discussed earlier. However, that high threshold may have borked the sorting ability of the model considering that the data is skewed towards positive cases now.

```{r cases_thresh_sixtyfive}
table(admissions$y_admit) %>%
  kable(col.names = c("Class", "Frequency")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

How about we check model performance with the threshold set at 0.5? For fun (and profit)!

```{r log_thresh_fifty}
# This code is dreadful. Sorry.
fifty_pred <- admissions %>%
  mutate(y_true = thresh_to_fac(prob_admit, threshold = .5)) %>%
  glm(basic_form, "binomial", data = .) %>%
  predict(type = "response") %>%
  thresh_to_fac(threshold = .5)

fifty_true <- thresh_to_fac(admissions$prob_admit, .5)
confusion_wrapper(fifty_true, fifty_pred)

rm(fifty_pred, fifty_true)
```

Using my handy dandy ~~notebook~~ _thresh_to_fac()_ function, I generated a set of predictions as factors where the positive class is defined as 0.51 and up.

My initial intuition from data exploration has been (somewhat) vindicated! I did not expect the model to perform so poorly with that change. Specificity is extremely low which is rather sensible considering the skewness of the classes. I'll keep 0.65 as a threshold.

```{r drop_prob_admit}
admissions <- admissions %>%
  select(-prob_admit)
```

## Bootstrap aggregated random forests [Baggin' CARTs]

```{r bagging_rf}
rf_traincontrol <- trainControl(method = "repeatedcv",
                                number = 5,
                                repeats = 5,
                                search = "random",
                                savePredictions = TRUE,
                                classProbs = TRUE,
                                summaryFunction = twoClassSummary,
                                allowParallel = TRUE)

# mtry_default <- floor(sqrt(ncol(admissions) - 1))
rf_grid <- expand.grid(mtry = 1:ncol(admissions),
                       splitrule = c("gini", "extratrees"),
                       min.node.size = c(1, 3, 5, 10))

rf_model_nopreproc <- train(y_admit ~ .,
                            method = "ranger",
                            trControl = rf_traincontrol,
                            tuneGrid = rf_grid,
                            metric = "ROC",
                            data = admissions)
```

The two highest rated answers on a question about [preprocessing for random forests](https://stats.stackexchange.com/questions/172842/best-practices-with-data-wrangling-before-running-random-forest-predictions) on Stack Overflow conflict. Wonderful! Random forests should theoretically be indifferent toward outliers or unimportant variables. One answer describes the algorithm as fairly "off the shelf," meaning that one may deploy a CART without much wrangling. Another answer explains that the theoretical prowess is sometimes wonky; all data should be seasoned a bit before cooking.

Scaled variables shouldn't harm nor help information gain at each cut of the tree, but I'll try the model both ways. Less complex models or modeling processes should always be preferred, of course, even if preprocessing doesn't strictly harm anything.

The hyperparameter **mtry** refers to the number of variables that may be possibly selected at each cut of the tree (this explanation may be off). By [default mtry](https://cran.r-project.org/web/packages/ranger/ranger.pdf) is set to the floor of the square root of the number of variables which is ~2 in this case.

I'm using **repeated cross validation** as the data are rather limited at `r nrow(admissions)` rows. I'm avoiding splitting the data into training and test sets for the first model due to that reason as well. I originally used **leave one out cross validation** which trains the model _N_ times. LOOCV is clearly computational expensive but provides very conservative estimates of model performance. However, I've found that LOOCV, CV, and repeated CV all offer same conclusions since the model is fairly stable. In other words, error doesn't vary wildly across folds. While I could stick with k fold cross validation, repeated CV seems to be good practice for a data set of this size.

### Diagnostics
```{r rf_no_preproc_diag}
rf_model_nopreproc$bestTune
plot(rf_model_nopreproc)

rf_y_pred_nopre <- predict(rf_model_nopreproc)
confusion_wrapper(admissions$y_admit, rf_y_pred_nopre)
```

## Random forests with preprocessing

```{r bagging_preproc_rf, warning=FALSE}
train_index <- createDataPartition(admissions$y_admit, p = 0.9)$Resample1

X_train <- admissions %>%
  slice(train_index) %>%
  select(-y_admit)

y_train <- admissions %>%
  slice(train_index) %>%
  select(y_admit) %>%
  unlist()

X_test <- admissions %>%
  slice(-train_index) %>%
  select(-y_admit)

y_test <- admissions %>%
  slice(-train_index) %>%
  select(y_admit) %>%
  unlist()

rf_model_preproc <- train(x = X_train,
                          y = y_train,
                          method = "ranger",
                          preProcess = c("scale", "pca"),
                          trControl = rf_traincontrol,
                          tuneGrid = rf_grid,
                          metric = "ROC")
```
The model crashes a few times with: `Error: mtry can not be larger than number of variables in data. Ranger will EXIT now`

Principal Component Analysis (PCA) reduces the total number of variables, so _ranger_ logically throws errors for the cases were the _mtry_ hyperparameter exceeds the number of columns.


### Diagnostics
```{r rf_preproc_diagnostics}
rf_ypred_preproc <- relevel(predict(rf_model_preproc, newdata = X_test), "No")
confusion_wrapper(y_test, rf_ypred_preproc)
```
```{r stop_cluster}
stopCluster(cluster_mp)
```
