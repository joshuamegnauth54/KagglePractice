---
title: "Kaggle Admissions2 Data Test"
author: "Joshua Megnauth"
output:
  html_document:
    theme: darkly
    highlight: zenburn
    df_print: paged
---

## Introduction
```{r load_libraries_data, warning=FALSE, message=FALSE}
library(kableExtra)
library(caret)
library(tidyverse)
library(ggplot2)
library(broom)

# Dracula colors
# https://draculatheme.com/
# Obviously not following the theme yet.
PINK <- "#ff79c6"
CYAN <- "#8be9fd"
GREEN <- "#50fa7b"
PURPLE <- "#bd93f9"
RED <- "#ff5555"
BACKGROUND <- "#282a36"

# Threshold for the outcomes (i.e. above THRESHOLD = a success)
THRESHOLD <- 0.65

admissions_name <- "Admission_Predict_Ver1.1.csv"
admissions_cols <- c("id", "gre", "toefl", "uni_ratings", "statement",
                     "letter", "cgpa", "research", "prob_admit")
admissions_types <- "iiifdddld"

admissions <- 
  read_csv(admissions_name, col_names = admissions_cols,
           col_types = admissions_types, skip = 1) %>%
  select(-id)

options(scipen = 16)
rm(admissions_name, admissions_cols, admissions_types)
```

The [graduate admissions data](https://www.kaggle.com/mohansacharya/graduate-admissions) data set is not immediately attractive at first glance. The data set is sparse, utilitarian, and tractable. Utility is a function of the data's admittedly humdrum goal of predicting graduate school admissions from dreary tests. However, the data set is also yielding due to the directness of the variables and the lack of cleaning required.

For those reasons I had been drawn to this data as a first, small project to play around with in R and Python.

Let's quickly go over the code above. **THRESHOLD** is a constant to separate the positive predictions from the negative---that is, admissions from non-admissions. I selected 0.65 as a probability after exploring the data below which means that the value is a judgement call rather than a science. As a parameter, a statistician or data scientist may tune the threshold for separating the classes. Finally, .65 is hardly perfect as we'll see below.

Anyway, the code to load the data isn't all that interesting. I manually skipped the header row, renamed the columns, and set the types.

## Exploring the data
```{r tests_plots}
standardize <- function(variable) {
  (variable - mean(variable))/sd(variable)
}

thresh_to_fac <- function(data, threshold = THRESHOLD) {
  as.factor(data > threshold)
}

train_test_split <- function(X, y, test_size = .2) {
  len <- length(X)
  rows <- sample(len)
  split <- test_size * len
  
  X_test <- X[1:split]
  y_test <- y[1:split]

  X_train <- X[split + 1:len]
  y_train <- y[split + 1:len]
  
  list(X_train, y_train, X_test, y_test)
}

admissions %>%
  mutate(gre_stdize = standardize(gre),
         toefl_stdize = standardize(toefl)) %>%
  ggplot() +
  geom_histogram(aes(gre_stdize), binwidth = .25,
                 color = BACKGROUND, fill = GREEN) +
  geom_histogram(aes(toefl_stdize), binwidth = .25,
                 color = BACKGROUND, fill = PINK, alpha = 0.5) +
  xlab("GRE and TOEFL (standardized)") +
  ylab("Frequency") +
  ggtitle("Standardized GRE and TOEFL scores")
```

$$z = \frac{x - \mu}{\sigma}$$

[to do]
Explain the standardization formula.

```{r probability_admission}
admissions %>%
  ggplot() +
  geom_density(aes(prob_admit, fill = research), alpha = 0.5) +
  scale_fill_manual(values = c(PINK, CYAN)) +
  labs(fill = "Research") +
  xlab("Admission") +
  ylab("Probability") +
  ggtitle("Probability of admission by research experience")
```

[notes]: probability with no research looks normal while both are distributed beyond 50%

```{r}
admissions %>%
  select(gre, toefl, statement, letter) %>%
  cor() %>%
  kable(col.names = c("GRE", "TOEFL", "SoP", "LoR")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

admissions %>%
  select(gre, toefl, statement, letter) %>%
  cor() %>%
  data.frame() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## A basic model

```{r naive_glm}
admissions <- admissions %>%
  mutate(y_admit = thresh_to_fac(prob_admit)) %>%
  select(-prob_admit)

basic_form <- y_admit ~ gre + uni_ratings + cgpa + statement + letter + research
basic_mod <- glm(basic_form, "binomial", admissions)

tidy(basic_mod, conf.int = TRUE, exponentiate = TRUE)
```

My first model is conservative and straightforward. I included only one of the tests as the two test scores are correlated. Of course, a person/observation that has a certain score on one test is likely to have a similar grade on the other test. Likewise, G.P.A. shouldn't deviate too wildly from TOEFL or GRE scores, but I included the variable in this preliminary model to be safe.

The coefficients for the odds fall into the confidence intervals. The confidence intervals are a bit wonky. For example, the confidence intervals for _university ratings_ are rather wide. P-values measure the probability of results at least as extreme as what was observed given that the null hypothesis is true. Only GRE scores, cumulative G.P.A., and letters of recommendation scores may not be due to random chance given $\alpha = .05$.

### Diagnostics

Let's test predictive ability. I fit my basic model on all of the data which in turn will be used for prediction. Of course, I realize that precluding a hold out set as well as using training data for prediction are anti-patterns, but the model above is basic and experimentation is fun!

```{r basic_model_test}
basic_preds <- thresh_to_fac(predict(basic_mod, type = "response"))
basic_confusion <- confusionMatrix(admissions$y_admit, reference = basic_preds,
                                   positive = "TRUE")

basic_confusion$table %>%
  kable(caption = "Confusion matrix: columns = true") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE)

tidy(basic_confusion) %>%
  select(term, estimate) %>%
  kable(col.names = c("Classification Function", "Value")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

My basic model performs _okay_ considering I conservatively hand picked variables. With that said, I'll repeat that I _tested_ on my _training_ data so the model may be overfit with only decent performance. Total accuracy is the ratio of correctly separated true positive and true negatives over all of the classes total. Accuracy's major problem is that the two classes may be unbalanced. The canonical example considers the test for an affliction where most test results are negative. Let's say that 99% of the cases in a data set for some condition are negative. A classifier may achieve an accuracy score of 99% while predicting **literally every positive case incorrectly.**

The logistic model above achieves an accuracy of 86% with a sensitivity (true positive rate) of 93.6%. Specificied, or correctly sorting false cases, is only 73.4%. The confusion matrix above shows 20 false negatives and 50 false positives. I set a threshold of 0.65 as discussed earlier. However, that high threshold probably borked the sorting ability of the model considering that the data is skewed towards positive cases now.

```{r }
